# -*- coding: utf-8 -*-
# KRKeywordClassic v2 â€” YAML ì™¸ë¶€ ì‚¬ì „ ë¡œë”© (Kiwi ëª…ì‚¬/ë³µí•©ì–´ + PMI)
# - í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸: binary set (ê°€ì¤‘ì¹˜ ì—†ìŒ, ê³ ì • ë³´ë„ˆìŠ¤)
# - Stopwords: ê³µí†µ ë¦¬ìŠ¤íŠ¸
# - Join rules: ì˜ˆì™¸ ëª‡ ê°€ì§€ë§Œ ìˆ˜ë™, ë‚˜ë¨¸ì§€ëŠ” í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ìë™ ë³‘í•©

import os
import re
import math
import yaml
from collections import Counter, defaultdict
from typing import List, Tuple
from kiwipiepy import Kiwi

# =============================================================================
# 0) ì •ê·œì‹ íŒ¨í„´
# =============================================================================
RE_ALLCAPS = re.compile(r"^[A-Z]{2,}$")
RE_MIXED   = re.compile(r"^(?:[A-Za-z]+\d+|\d+[A-Za-z]+)$")
RE_UNIT    = re.compile(r"^\d+\s?(?:g|kg|ml|l|ê°œ|íŒ©|ë´‰|ë°•ìŠ¤|ìº”|ë³‘|ë¬¶ìŒ|ì„¸íŠ¸|ë§¤|ì¥|gx|x\d+)$", re.I)
RE_HAN     = re.compile(r"[ê°€-í£]")
RE_VALID   = re.compile(r"[ê°€-í£A-Za-z0-9]+")

# ì •ê·œí™”(ê³µë°± ì¶•ì•½ ë“±)
CANON_REPLACEMENTS = [("  ", " ")]

def canon_form(s: str) -> str:
    s = s.strip().lower()
    for a, b in CANON_REPLACEMENTS:
        while a in s:
            s = s.replace(a, b)
    # í•œê¸€ë§Œì¸ ê²½ìš° ê³µë°± ì œê±° ë²„ì „ ì‚¬ìš©
    if RE_HAN.search(s) and not re.search(r"[A-Za-z0-9]", s):
        return s.replace(" ", "")
    return s

# =============================================================================
# 1) YAML ë¡œë”
# =============================================================================
DICT_DIR = os.environ.get("DICT_DIR", os.path.join(os.path.dirname(__file__), "dicts"))

STOPWORDS_SET: set[str] = set()
WHITELIST_SET: set[str] = set()
JOIN_RULES: dict[tuple[str, ...], str] = {}

# í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ëŒ€(ê°€ì¤‘ì¹˜ ëŒ€ì‹  ê³ ì • ë³´ë„ˆìŠ¤)
WL_BONUS = 1.0
FORCE_INCLUDE_WHITELIST = True  # ë¹ˆë„/PMI ì—†ì–´ë„ ìƒë‹¨ ë…¸ì¶œì— ìœ ë¦¬í•˜ê²Œ

def _read_yaml(path: str) -> dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def load_dictionaries(dict_dir: str = DICT_DIR):
    """dicts/stopwords.yaml, whitelist.yaml, join_rules.yaml ë¡œë“œ"""
    global STOPWORDS_SET, WHITELIST_SET, JOIN_RULES

    # stopwords.yaml
    sw = _read_yaml(os.path.join(dict_dir, "stopwords.yaml"))
    STOPWORDS_SET = set(map(str, (sw.get("common") or [])))

    # whitelist.yaml
    wl = _read_yaml(os.path.join(dict_dir, "whitelist.yaml"))
    WHITELIST_SET = set()
    for section in ("food", "brand", "place", "generic"):
        for tok in (wl.get(section) or []):
            # ë¬¸ìì—´ë§Œ ì§€ì› (ì‹¬í”Œ ìŠ¤í‚¤ë§ˆ)
            if isinstance(tok, str):
                WHITELIST_SET.add(tok.strip())
            else:
                # í˜¹ì‹œ ê°ì²´ê°€ ë“¤ì–´ì™€ë„ token í‚¤ë§Œ ì‚¬ìš© (í˜¸í™˜)
                token = str(tok.get("token", "")).strip()
                if token:
                    WHITELIST_SET.add(token)

    # join_rules.yaml
    jr = _read_yaml(os.path.join(dict_dir, "join_rules.yaml"))
    JOIN_RULES = {}
    for r in (jr.get("rules") or []):
        inp = tuple(map(str, r.get("in") or []))
        out = str(r.get("out") or "").strip()
        if inp and out:
            JOIN_RULES[inp] = out

# ìµœì´ˆ 1íšŒ ë¡œë“œ
load_dictionaries()

# =============================================================================
# 2) ìœ í‹¸
# =============================================================================
def whitelist_hit(tok: str) -> bool:
    """ì •ê·œí˜• í† í°ì´ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ ì—¬ë¶€"""
    return tok in WHITELIST_SET

def is_noise_token(tok: str) -> bool:
    """ë¶ˆìš©ì–´/íŒ¨í„´ ì»·"""
    t = tok.strip().lower()
    if len(t) <= 1 and not t.isdigit(): return True
    if t in STOPWORDS_SET: return True
    if RE_ALLCAPS.match(t): return True
    if RE_MIXED.match(t): return True
    if RE_UNIT.match(t): return True
    if not RE_VALID.search(t): return True
    return False

def brand_like(tok: str) -> bool:
    """í•œê¸€ í¬í•¨ & ê¸¸ì´>=2 â†’ ê³ ìœ ëª…/ì¼ë°˜ëª… ê²½í–¥ ë³´ë„ˆìŠ¤"""
    return bool(RE_HAN.search(tok)) and (len(tok) >= 2)

# =============================================================================
# 3) í† í°í™” & ë³‘í•©
# =============================================================================
def tokenize_kiwi(kiwi: Kiwi, text: str) -> List[str]:
    """Kiwië¡œ ëª…ì‚¬/ì™¸ë˜ì–´ ìœ„ì£¼ í† í°í™”"""
    text = text.replace("\u200b", " ").replace("\ufeff", " ")
    text = re.sub(r"\s+", " ", text).strip()
    toks: List[str] = []
    for sent in kiwi.analyze(text):
        for t in sent[0]:
            if t.tag.startswith(("NN",)) or t.tag == "SL":
                w = t.form.strip()
                if w and RE_VALID.search(w):
                    toks.append(w)
    return toks

def apply_join_rules(tokens: List[str]) -> List[str]:
    """YAMLì— ì •ì˜ëœ ì˜ˆì™¸ì  ë³‘í•© ê·œì¹™ ì ìš© (ê°€ë³€ ê¸¸ì´ ì§€ì›)"""
    i, out = 0, []
    L = len(tokens)
    max_n = max((len(k) for k in JOIN_RULES.keys()), default=2)
    while i < L:
        matched = False
        for n in range(min(max_n, L - i), 1, -1):
            tup = tuple(tokens[i:i+n])
            if tup in JOIN_RULES:
                out.append(JOIN_RULES[tup])
                i += n
                matched = True
                break
        if not matched:
            out.append(tokens[i])
            i += 1
    return out

def auto_join_by_whitelist(tokens: List[str]) -> List[str]:
    """ì—°ì† í† í°(ìµœëŒ€ 4ê·¸ë¨)ì„ ë¶™ì—¬ë³¸ ê²°ê³¼ê°€ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ë©´ ìë™ ë³‘í•©"""
    out, i, L = [], 0, len(tokens)
    while i < L:
        merged = None
        for n in range(min(4, L - i), 1, -1):
            cand = "".join(tokens[i:i+n])
            if cand in WHITELIST_SET:
                merged = cand
                i += n
                break
        if merged:
            out.append(merged)
        else:
            out.append(tokens[i])
            i += 1
    return out

def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:
    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

# =============================================================================
# 4) í•µì‹¬ ë¡œì§
# =============================================================================
def best_keywords(text: str, top_k: int = 15,
                  min_unigram: int = 2, min_bigram: int = 2,
                  pmi_floor: float = 0.2) -> List[Tuple[str, float]]:
    kiwi = Kiwi()

    # 1) í† í°í™” + ë¶ˆìš©ì–´ ì»·
    raw_tokens = tokenize_kiwi(kiwi, text)
    raw_tokens = [w for w in raw_tokens if not is_noise_token(w)]

    # 2) ë³‘í•© (ì˜ˆì™¸ ê·œì¹™ â†’ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ìë™ ë³‘í•©)
    tokens = apply_join_rules(raw_tokens)
    tokens = auto_join_by_whitelist(tokens)
    tokens = [w for w in tokens if not is_noise_token(w)]
    if not tokens:
        return []

    # 3) ìœ ë‹ˆê·¸ë¨/ë°”ì´ê·¸ë¨ ë¹ˆë„
    uni = Counter(tokens)
    bi = Counter([
        bg for bg in build_ngrams(tokens, 2)
        if not (is_noise_token(bg[0]) or is_noise_token(bg[1]))
    ])

    # 4) ìµœì†Œ ë¹ˆë„ í•„í„° (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ëŠ” ì˜ˆì™¸ í—ˆìš©)
    uni = Counter({w: c for w, c in uni.items() if c >= min_unigram or whitelist_hit(w)})
    bi = Counter({
        bg: c for bg, c in bi.items()
        if c >= min_bigram or whitelist_hit(bg[0]) or whitelist_hit(bg[1])
    })

    total_uni = sum(uni.values()) or 1
    total_bi = sum(bi.values()) or 1

    # 5) PMI ê³„ì‚° (í•˜í•œì„  ì ìš©)
    pmi = {}
    for (w1, w2), c12 in bi.items():
        p_w1 = uni.get(w1, 0) / total_uni
        p_w2 = uni.get(w2, 0) / total_uni
        p_w1w2 = c12 / total_bi
        score = 0.0
        if p_w1 > 0 and p_w2 > 0 and p_w1w2 > 0:
            score = math.log2(p_w1w2 / (p_w1 * p_w2))
        if score >= pmi_floor:
            pmi[(w1, w2)] = score

    # 6) ìŠ¤ì½”ì–´ë§
    cand_scores: dict[str, float] = defaultdict(float)

    # (a) ë°”ì´ê·¸ë¨: ë¹ˆë„ + PMI + ë³´ë„ˆìŠ¤/í˜ë„í‹° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ëŒ€
    for (w1, w2), c12 in bi.items():
        phrase = f"{w1} {w2}"
        allow_by_pmi = (w1, w2) in pmi
        allow_by_wl = whitelist_hit(w1) or whitelist_hit(w2) or whitelist_hit(phrase.replace(" ", ""))  # ë¶™ì¸ í‘œë©´í˜•ë„ í™•ì¸

        if not allow_by_pmi and not allow_by_wl:
            continue

        s = 0.0
        s += c12 * 1.0
        if allow_by_pmi:
            s += pmi.get((w1, w2), 0.0) * 1.3

        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë³´ë„ˆìŠ¤
        if allow_by_wl:
            s += WL_BONUS
            if FORCE_INCLUDE_WHITELIST:
                s = max(s, WL_BONUS + 0.5)

        if brand_like(w1): s += 0.2
        if brand_like(w2): s += 0.2
        if not RE_HAN.search(phrase): s -= 0.8

        cand_scores[phrase] = max(cand_scores.get(phrase, 0.0), s)

    # (b) ìœ ë‹ˆê·¸ë¨: ë¹ˆë„ + ë³´ë„ˆìŠ¤/í˜ë„í‹° + í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ìš°ëŒ€
    for w, c in uni.items():
        s = c * 0.7
        if brand_like(w): s += 0.2
        if not RE_HAN.search(w): s -= 0.6

        if whitelist_hit(w):
            s += WL_BONUS
            if FORCE_INCLUDE_WHITELIST:
                s = max(s, WL_BONUS + 0.2)

        cand_scores[w] = max(cand_scores.get(w, 0.0), s)

    # 7) í‘œë©´í˜• í†µí•© (â€œì´ˆì½” ì†¡ì´â€ vs â€œì´ˆì½”ì†¡ì´â€)
    fused: dict[str, Tuple[str, float]] = {}
    for k, s in cand_scores.items():
        key = canon_form(k)
        if key not in fused or s > fused[key][1]:
            fused[key] = (k, s)

    ranked = sorted(fused.values(), key=lambda x: x[1], reverse=True)

    # 8) ì¤‘ë³µ ì–µì œ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ëŠ” ìš°ì„  í†µê³¼)
    selected: List[Tuple[str, float]] = []
    used_parts = set()
    for kw, sc in ranked:
        parts = tuple(kw.split())
        wl_hit = whitelist_hit(kw.replace(" ", "")) or any(whitelist_hit(p) for p in parts)
        if not wl_hit:
            # (í•œê¸€ ê¸°ì¤€) êµ¬ì„±ì–´ê°€ ì „ë¶€ ì´ë¯¸ ì‚¬ìš©ë˜ë©´ ìŠ¤í‚µ
            if all(canon_form(p) in used_parts for p in parts if RE_HAN.search(p)):
                continue
        selected.append((kw, sc))
        for p in parts:
            used_parts.add(canon_form(p))
        if len(selected) >= top_k:
            break

    return selected

# =============================================================================
# CLI
# =============================================================================
if __name__ == "__main__":
    print("ğŸ§  í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê³  ì—”í„° 2ë²ˆìœ¼ë¡œ ì¢…ë£Œí•˜ì„¸ìš”.")
    print(f"ğŸ“‚ DICT_DIR = {DICT_DIR}")
    lines = []
    while True:
        try:
            line = input()
        except EOFError:
            break
        if not line.strip():
            break
        lines.append(line)
    text = "\n".join(lines).strip()

    kws = best_keywords(text, top_k=15)
    if not kws:
        print("í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâœ… í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ):")
        for k, s in kws:
            print(f"  {k:25s} {s:.3f}")
