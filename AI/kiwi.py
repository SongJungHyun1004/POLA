# -*- coding: utf-8 -*-
# KRKeywordClassic v2 â€” Kiwi only (ëª…ì‚¬/ë³µí•©ì–´ + PMI), ì¡ìŒ ê°•ì œ ì œê±° & ë³‘í•© ê°•í™”

import re
import math
from collections import Counter, defaultdict
from typing import List, Tuple
from kiwipiepy import Kiwi

# -----------------------------
# 0) ì‚¬ì „/íŒ¨í„´
# -----------------------------
STOPWORDS = {
    # UI/í”Œë«í¼ ì¡ìŒ
    "ì´ë¯¸ì§€","ì‚¬ì§„","ê·¸ë¦¼","í™”ë©´","ìŠ¤í¬ë¦°ìƒ·","ìƒë‹¨","í•˜ë‹¨","ì˜¤ë¥¸ìª½","ì™¼ìª½",
    "ìƒíƒœ","í‘œì‹œì¤„","ë°°í„°ë¦¬","ì‹œê°„","ì™€ì´íŒŒì´","wifi","wi","fi","ì—°ê²°",
    "ì‚¬ìš©ì","ì¸í„°í˜ì´ìŠ¤","ìš”ì†Œ","í…ìŠ¤íŠ¸","ê·¸ë˜í”½","ìœ„ì¹˜","ì •ë³´","ì œì™¸","ì¶”ê°€",
    "ëª¨ë°”ì¼","íœ´ëŒ€í°","ì•±","ì˜¨ë¼ì¸","ì‡¼í•‘","ê²½í—˜","í‘œì‹œ","íŒ”ë¡œìš°","ì•„ì´ë””",
    "url","http","https","ë¦¬ë·°","ì°¸ì¡°","ìƒì„¸ì„¤ëª…","í˜„ì¬","íŒë§¤ì¤‘","íŒë§¤","í˜œíƒ","í”ŒëŸ¬ìŠ¤",
    "ê°€ì„","í•œì •","ì›ì‚°ì§€","ìµœëŒ€","ì ë¦½","í¬ì¸íŠ¸","ì›","ê´‘ê³ ","ë¼ë²¨","ë¡œê³ ",
    # ìˆ«ìì„± í† í°ì´ë‘ ë¶™ìœ¼ë©´ ë…¸ì´ì¦ˆ ìœ ë°œ
    "ad","hd","kt","talk","km"
}

# í† í° ë‚´ë¶€ì— ë³´ì´ë©´ ì¡ìŒ ì·¨ê¸‰ (ì•½ì–´/íŒ¨ë„/ê³„ê¸°íŒ ìˆ«ì)
RE_ALLCAPS = re.compile(r"^[A-Z]{2,}$")
RE_MIXED   = re.compile(r"^(?:[A-Za-z]+\d+|\d+[A-Za-z]+)$")
RE_UNIT    = re.compile(r"^\d+\s?(?:g|kg|ml|l|ê°œ|íŒ©|ë´‰|ë°•ìŠ¤|ìº”|ë³‘|ë¬¶ìŒ|ì„¸íŠ¸|ë§¤|ì¥|gx|x\d+)$", re.I)
RE_HAN     = re.compile(r"[ê°€-í£]")
RE_VALID   = re.compile(r"[ê°€-í£A-Za-z0-9]+")

# í”í•œ â€˜ë¶„ë¦¬ í‘œê¸°â€™ë¥¼ ë¶™ì—¬ì£¼ëŠ” ë³‘í•© ê·œì¹™(í•„ìš”ì‹œ ê³„ì† ì¶”ê°€)
JOIN_RULES = {
    ("ì´ˆì½”","ì†¡ì´"): "ì´ˆì½”ì†¡ì´",
    ("ì´ˆì½”ì¹©","ì¿ í‚¤"): "ì´ˆì½”ì¹© ì¿ í‚¤",
    ("ì• í”Œ","íŒŒì´"): "ì• í”ŒíŒŒì´",
    ("ë°€í¬","í‹°"): "ë°€í¬í‹°",
    ("í¬ë¡œ","ì•„ìƒ"): "í¬ë¡œì•„ìƒ",
    ("ë¸ë¦¬","í”„ë‘ìŠ¤"): "ë¸ë¦¬í”„ë‘ìŠ¤",
    ("ë¡œì–„","ë°€í¬í‹°"): "ë¡œì–„ ë°€í¬í‹°",
}

# í•œê¸€ í‘œê¸° ë³€í˜• ì •ê·œí™”(ê³µë°± ì œê±° ê¸°ì¤€)
CANON_REPLACEMENTS = [
    ("  ", " "),
]

def canon_form(s: str) -> str:
    s = s.strip().lower()
    for a,b in CANON_REPLACEMENTS:
        while a in s: s = s.replace(a,b)
    # í•œê¸€ë§Œì¸ ê²½ìš°ëŠ” ê³µë°± ì œê±°í•œ ë²„ì „ë„ í‚¤ë¡œ ì”€
    if RE_HAN.search(s) and not re.search(r"[A-Za-z0-9]", s):
        return s.replace(" ","")
    return s

def is_noise_token(tok: str) -> bool:
    t = tok.strip().lower()
    if len(t) <= 1 and not t.isdigit(): return True
    if t in STOPWORDS: return True
    if RE_ALLCAPS.match(t): return True
    if RE_MIXED.match(t): return True
    if RE_UNIT.match(t): return True
    if not RE_VALID.search(t): return True
    return False

def brand_like(tok: str) -> bool:
    # ë¸Œëœë“œ/ìƒí’ˆ íŒíŠ¸(ëŒ€ë¬¸ì/ì˜ë¬¸í˜¼í•©ì€ ë…¸ì´ì¦ˆë„ ë˜ì§€ë§Œ, í•œê¸€+ê³ ìœ ëª… ê²½í–¥ ë³´ë„ˆìŠ¤ë§Œ ì•½í•˜ê²Œ)
    return bool(RE_HAN.search(tok)) and (len(tok) >= 2)

# -----------------------------
# 1) í† í°í™” (ëª…ì‚¬/ê³ ìœ ëª…ì‚¬/ì™¸ë˜ì–´)
# -----------------------------
def tokenize_kiwi(kiwi: Kiwi, text: str) -> List[str]:
    text = text.replace("\u200b"," ").replace("\ufeff"," ")
    text = re.sub(r"\s+", " ", text).strip()
    toks: List[str] = []
    for sent in kiwi.analyze(text):
        for t in sent[0]:
            if t.tag.startswith(("NN",)) or t.tag == "SL":
                w = t.form.strip()
                if w and RE_VALID.search(w):
                    toks.append(w)
    return toks

# ì¸ì ‘ í† í° ë³‘í•© ê·œì¹™ ì ìš©
def apply_join_rules(tokens: List[str]) -> List[str]:
    i, out = 0, []
    L = len(tokens)
    while i < L:
        if i < L-1 and (tokens[i], tokens[i+1]) in JOIN_RULES:
            out.append(JOIN_RULES[(tokens[i], tokens[i+1])])
            i += 2
        else:
            out.append(tokens[i])
            i += 1
    return out

def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str,...]]:
    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

# -----------------------------
# 2) í•µì‹¬ ë¡œì§
# -----------------------------
def best_keywords(text: str, top_k: int = 15,
                  min_unigram=2, min_bigram=2,
                  pmi_floor=0.2) -> List[Tuple[str, float]]:
    kiwi = Kiwi()
    # 1) í† í°í™” + ë¶ˆìš©ì–´ ì»·
    raw_tokens = tokenize_kiwi(kiwi, text)
    raw_tokens = [w for w in raw_tokens if not is_noise_token(w)]

    # 2) ì¸ì ‘ ë³‘í•©(ì´ˆì½” ì†¡ì´â†’ì´ˆì½”ì†¡ì´ ë“±)
    tokens = apply_join_rules(raw_tokens)
    tokens = [w for w in tokens if not is_noise_token(w)]
    if not tokens:
        return []

    # 3) ìœ ë‹ˆê·¸ë¨/ë°”ì´ê·¸ë¨ ë¹ˆë„
    uni = Counter(tokens)
    bi  = Counter([bg for bg in build_ngrams(tokens, 2)
                   if not (is_noise_token(bg[0]) or is_noise_token(bg[1]))])

    # ìµœì†Œ ë¹ˆë„ í•„í„°(ì¡ìŒ ê²°í•© ì–µì œ)
    uni = Counter({w:c for w,c in uni.items() if c >= min_unigram})
    bi  = Counter({bg:c for bg,c in bi.items() if c >= min_bigram})

    total_uni = sum(uni.values()) or 1
    total_bi  = sum(bi.values()) or 1

    # 4) PMI ê³„ì‚° (í•˜í•œì„  ì ìš©)
    pmi = {}
    for (w1,w2), c12 in bi.items():
        p_w1 = uni.get(w1,0)/total_uni
        p_w2 = uni.get(w2,0)/total_uni
        p_w1w2 = c12/total_bi
        score = 0.0
        if p_w1>0 and p_w2>0 and p_w1w2>0:
            score = math.log2(p_w1w2/(p_w1*p_w2))
        if score >= pmi_floor:
            pmi[(w1,w2)] = score

    # 5) ìŠ¤ì½”ì–´ë§
    cand_scores: dict[str,float] = defaultdict(float)

    # (a) ë°”ì´ê·¸ë¨ ìš°ì„ : ë¹ˆë„ + PMI + ë³´ë„ˆìŠ¤/í˜ë„í‹°
    for (w1,w2), c12 in bi.items():
        if (w1,w2) not in pmi:
            continue  # PMI í•˜í•œ ë¯¸ë‹¬ ì»·
        phrase = f"{w1} {w2}"
        score = 0.0
        score += c12 * 1.0                     # ë¹ˆë„
        score += pmi[(w1,w2)] * 1.3            # ê²°í•© ë³´ë„ˆìŠ¤
        if brand_like(w1): score += 0.2
        if brand_like(w2): score += 0.2

        # ì˜ë¬¸ ì•½ì–´/ìˆ«ì ìœ„ì£¼ë©´ í˜ë„í‹°
        if not RE_HAN.search(phrase):
            score -= 0.8
        cand_scores[phrase] = max(cand_scores[phrase], score)

    # (b) ìœ ë‹ˆê·¸ë¨: ë°”ì´ê·¸ë¨ ëŒ€ë¹„ ë‚®ê²Œ ë°˜ì˜
    for w, c in uni.items():
        score = c * 0.7
        if brand_like(w): score += 0.2
        if not RE_HAN.search(w):
            score -= 0.6
        cand_scores[w] = max(cand_scores[w], score)

    # 6) ì •ê·œí™” í‚¤ë¡œ ì¤‘ë³µ/ë³€í˜• í†µí•© (â€œì´ˆì½” ì†¡ì´â€ vs â€œì´ˆì½”ì†¡ì´â€)
    fused: dict[str, Tuple[str,float]] = {}  # canon_key -> (repr, score)
    for k, s in cand_scores.items():
        key = canon_form(k)
        if key not in fused or s > fused[key][1]:
            fused[key] = (k, s)

    # 7) ìµœì¢… ì •ë ¬ + â€˜êµ¬ì„±ì–´ í¬í•¨ë„â€™ë¡œ ì¤‘ë³µ ì–µì œ
    ranked = sorted(fused.values(), key=lambda x: x[1], reverse=True)
    selected: List[Tuple[str,float]] = []
    used_parts = set()

    for kw, sc in ranked:
        parts = tuple(kw.split())
        # (í•œê¸€ ê¸°ì¤€) êµ¬ì„±ì–´ê°€ ì „ë¶€ ì´ë¯¸ ì‚¬ìš©ë˜ë©´ ìŠ¤í‚µ
        if all(canon_form(p) in used_parts for p in parts if RE_HAN.search(p)):
            continue
        selected.append((kw, sc))
        for p in parts:
            used_parts.add(canon_form(p))
        if len(selected) >= top_k:
            break

    return selected

# -----------------------------
# CLI
# -----------------------------
if __name__ == "__main__":
    print("ğŸ§  í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê³  ì—”í„° 2ë²ˆìœ¼ë¡œ ì¢…ë£Œí•˜ì„¸ìš”.")
    lines = []
    while True:
        try:
            line = input()
        except EOFError:
            break
        if not line.strip():
            break
        lines.append(line)
    text = "\n".join(lines).strip()

    kws = best_keywords(text, top_k=15)
    if not kws:
        print("í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâœ… í‚¤ì›Œë“œ (ìƒìœ„ 15ê°œ):")
        for k, s in kws:
            print(f"  {k:25s} {s:.3f}")
